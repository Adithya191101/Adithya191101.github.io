<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="RL Based Robotic Arm Trajectory Planning - Hybrid PPO, SMDP, and MPC approach for obstacle avoidance by Adithya Rajendran">
    <meta name="author" content="Adithya Rajendran">

    <title>RL Based Robotic Arm Trajectory Planning | Adithya Rajendran</title>

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">

    <!-- Lucide Icons -->
    <script src="https://unpkg.com/lucide@latest"></script>

    <!-- GSAP for animations -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.12.2/gsap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.12.2/ScrollTrigger.min.js"></script>

    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="project-detail.css">
</head>
<body>
    <!-- Navigation -->
    <header class="nav-header" id="navbar">
        <nav class="nav-container">
            <a href="index.html" class="nav-logo">
                <span class="logo-text">AR</span>
            </a>

            <button class="mobile-menu-btn" id="mobileMenuBtn" aria-label="Toggle menu">
                <i data-lucide="menu" class="menu-icon"></i>
                <i data-lucide="x" class="close-icon"></i>
            </button>

            <ul class="nav-links" id="navLinks">
                <li><a href="index.html#about">About</a></li>
                <li><a href="index.html#skills">Skills</a></li>
                <li><a href="index.html#experience">Experience</a></li>
                <li><a href="projects.html" class="active">Projects</a></li>
                <li><a href="index.html#education">Education</a></li>
                <li><a href="honors.html">Honors</a></li>
                <li><a href="publications.html">Publications</a></li>
                <li><a href="contact.html">Contact</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <!-- Project Hero -->
        <section class="project-detail-hero">
            <div class="container">
                <a href="projects.html" class="back-link">
                    <i data-lucide="arrow-left"></i>
                    Back to Projects
                </a>

                <h1>RL Based Robotic Arm Trajectory Planning</h1>
                <p class="project-tagline" style="color: var(--text-secondary); font-size: 1.1rem; max-width: 800px;">
                    A hybrid reinforcement learning approach combining Proximal Policy Optimization (PPO), Semi-Markov Decision Processes (SMDP), and Model Predictive Control (MPC) for robotic arm trajectory planning with obstacle avoidance.
                </p>

                <div class="project-meta">
                    <div class="project-meta-item">
                        <i data-lucide="calendar"></i>
                        <span>December 2024</span>
                    </div>
                    <div class="project-meta-item">
                        <i data-lucide="building"></i>
                        <span>Northeastern University</span>
                    </div>
                    <div class="project-meta-item">
                        <i data-lucide="book-open"></i>
                        <span>Reinforcement Learning Course</span>
                    </div>
                    <div class="project-meta-item">
                        <i data-lucide="users"></i>
                        <span>Barath Balamurugan, Adithya Rajendran</span>
                    </div>
                </div>

                <div class="cta-buttons">
                    <a href="https://github.com/Adithya191101/RL_final_project" target="_blank" rel="noopener" class="btn-github">
                        <i data-lucide="github"></i>
                        View on GitHub
                    </a>
                    <a href="assets/rl-arm/RL_Report.pdf" target="_blank" rel="noopener" class="btn-github">
                        <i data-lucide="file-text"></i>
                        View Report (PDF)
                    </a>
                </div>
            </div>
        </section>

        <!-- Project Content -->
        <section class="project-content">
            <div class="container">

                <!-- Overview -->
                <div class="project-section">
                    <h2><i data-lucide="info"></i> Project Overview</h2>
                    <p>
                        Robotic arm trajectory planning presents significant challenges, particularly in environments with obstacles, dynamic constraints, and precise target requirements. Traditional approaches often struggle with the complexity and high dimensionality of the problem space. This project addresses these challenges by integrating three powerful methodologies into a unified framework.
                    </p>
                    <p>
                        The system enables robotic arms to navigate complex environments, avoid obstacles, and reach target positions efficiently. We implement a PyBullet simulation environment with customizable physics parameters and a carefully designed reward function that encourages desirable behaviors.
                    </p>

                    <!-- Key Results -->
                    <div class="results-highlight">
                        <div class="result-stat">
                            <div class="value">83%</div>
                            <div class="label">Success Rate</div>
                            <div class="comparison">vs 42% (PPO alone)</div>
                        </div>
                        <div class="result-stat">
                            <div class="value">156.3</div>
                            <div class="label">Average Reward</div>
                            <div class="comparison">vs 67.2 (PPO alone)</div>
                        </div>
                        <div class="result-stat">
                            <div class="value">213</div>
                            <div class="label">Avg Episode Length</div>
                            <div class="comparison">vs 312 (PPO alone)</div>
                        </div>
                        <div class="result-stat">
                            <div class="value">2x</div>
                            <div class="label">Performance Gain</div>
                            <div class="comparison">Hybrid vs Baseline</div>
                        </div>
                    </div>

                    <div class="feature-highlight">
                        <h3><i data-lucide="target"></i> Key Contributions</h3>
                        <ul style="color: var(--text-secondary); margin: 0; padding-left: 1.5rem; line-height: 2;">
                            <li>Integrated RL framework combining PPO, SMDP, and MPC for robotic arm trajectory planning</li>
                            <li>Carefully designed reward function balancing goal achievement, obstacle avoidance, and motion efficiency</li>
                            <li>Curriculum learning approach that gradually increases task difficulty during training</li>
                            <li>Comprehensive comparative analysis of different control methodologies</li>
                        </ul>
                    </div>
                </div>

                <!-- System Architecture -->
                <div class="project-section">
                    <h2><i data-lucide="layout"></i> System Architecture</h2>
                    <p>
                        The system consists of three main components: Environment Simulation, Reinforcement Learning Agent, and Control System. The architecture enables seamless integration between high-level learning and low-level control.
                    </p>

                    <div class="project-image-card" onclick="openImageModal('assets/rl-arm/report_img_3_1.png', 'System Architecture - Execution Pipeline')" style="max-width: 800px; margin: 1.5rem auto;">
                        <img src="assets/rl-arm/report_img_3_1.png" alt="High-level execution pipeline">
                        <div class="caption">High-level execution pipeline showing training, evaluation, and benchmarking paths</div>
                    </div>

                    <h3><i data-lucide="box"></i> Environment (PyBullet)</h3>
                    <p>
                        The environment is built using PyBullet, an open-source physics simulator that provides accurate rigid-body dynamics and real-time visualization. Key components include:
                    </p>
                    <ul style="color: var(--text-secondary); margin: 0; padding-left: 1.5rem; line-height: 2;">
                        <li><strong>KUKA Robotic Arm:</strong> Industrial manipulator with 7 degrees of freedom</li>
                        <li><strong>Dynamic Obstacles:</strong> Randomly initialized with varying positions and sizes</li>
                        <li><strong>State Space:</strong> End-effector position, obstacle positions, goal position, and velocity (12D)</li>
                        <li><strong>Action Space:</strong> Macro-actions including "Move to Waypoint" and "Avoid Obstacle"</li>
                    </ul>

                    <div class="project-image-grid two-col" style="margin-top: 1.5rem;">
                        <div class="project-image-card" onclick="openImageModal('assets/rl-arm/report_img_6_1.png', '2D Visualization of RL Process')">
                            <img src="assets/rl-arm/report_img_6_1.png" alt="2D RL visualization">
                            <div class="caption">2D representation of the reinforcement learning process</div>
                        </div>
                        <div class="project-image-card" onclick="openImageModal('assets/rl-arm/report_img_6_2.png', '3D Visualization of RL Process')">
                            <img src="assets/rl-arm/report_img_6_2.png" alt="3D RL visualization">
                            <div class="caption">3D representation in PyBullet simulation environment</div>
                        </div>
                    </div>
                </div>

                <!-- Methodology -->
                <div class="project-section">
                    <h2><i data-lucide="cpu"></i> Methodology</h2>
                    <p>
                        Our approach combines three complementary methodologies, each addressing different aspects of the trajectory planning problem:
                    </p>

                    <div class="methodology-grid">
                        <div class="methodology-card">
                            <h4><i data-lucide="brain"></i> Proximal Policy Optimization (PPO)</h4>
                            <p>
                                A sample-efficient policy gradient algorithm that strikes a balance between performance and stability by constraining policy updates through a clipped objective function. This prevents the new policy from deviating too far from the previous one, reducing the risk of performance collapse.
                            </p>
                            <div class="properties">
                                <span class="property">Policy Gradient</span>
                                <span class="property">Clipped Objective</span>
                                <span class="property">Stable Learning</span>
                            </div>
                        </div>

                        <div class="methodology-card">
                            <h4><i data-lucide="clock"></i> Semi-Markov Decision Process (SMDP)</h4>
                            <p>
                                An extension of the traditional MDP framework that allows actions to take variable amounts of time. This enables macro-actions like "move to waypoint" that span multiple timesteps, leading to more realistic and efficient long-term planning in dynamic environments.
                            </p>
                            <div class="properties">
                                <span class="property">Temporal Abstraction</span>
                                <span class="property">Macro-Actions</span>
                                <span class="property">Variable Duration</span>
                            </div>
                        </div>

                        <div class="methodology-card">
                            <h4><i data-lucide="sliders"></i> Model Predictive Control (MPC)</h4>
                            <p>
                                A powerful control strategy that optimizes control actions by predicting future states over a finite time window. At each timestep, MPC solves an optimization problem subject to system dynamics and constraints, allowing continuous adaptation to environmental changes.
                            </p>
                            <div class="properties">
                                <span class="property">Receding Horizon</span>
                                <span class="property">Constraint Handling</span>
                                <span class="property">Real-time Precision</span>
                            </div>
                        </div>
                    </div>

                    <!-- PPO Training Flow -->
                    <h3><i data-lucide="refresh-cw"></i> PPO Training Flow</h3>
                    <p>
                        The PPO algorithm uses a clipped surrogate objective to ensure stable policy updates while maintaining sample efficiency.
                    </p>

                    <div class="project-image-card" onclick="openImageModal('assets/rl-arm/report_img_4_1.png', 'PPO Training Flow Diagram')" style="max-width: 800px; margin: 1.5rem auto;">
                        <img src="assets/rl-arm/report_img_4_1.png" alt="PPO Training Flow">
                        <div class="caption">Detailed PPO training flow showing interaction between agent, environment, and learning updates</div>
                    </div>

                    <div class="equation-box">
                        L_CLIP(θ) = E_t[min(r_t(θ)A_t, clip(r_t(θ), 1-ε, 1+ε)A_t)]
                    </div>
                    <p style="font-size: 0.9rem;">
                        Where r_t(θ) is the probability ratio between new and old policies, A_t is the estimated advantage, and ε is the clipping parameter (typically 0.2).
                    </p>
                </div>

                <!-- Reward Function -->
                <div class="project-section">
                    <h2><i data-lucide="award"></i> Reward Function Design</h2>
                    <p>
                        The reward function plays a central role in shaping the agent's behavior. We designed a composite reward signal with multiple components targeting specific aspects of desired behavior:
                    </p>

                    <div class="equation-box">
                        r = r_dist + r_prog + r_goal + r_time + r_coll + r_move + r_expl
                    </div>

                    <div style="margin-top: 1.5rem;">
                        <div class="reward-component">
                            <div class="symbol">r_dist</div>
                            <div class="description">
                                <strong>Distance Penalty (-10 * ||p_ee - p_goal||):</strong> Penalizes the Euclidean distance between end-effector and goal position, encouraging the arm to move closer to the target.
                            </div>
                        </div>

                        <div class="reward-component">
                            <div class="symbol">r_prog</div>
                            <div class="description">
                                <strong>Progress Reward (30 * (d_prev - d_curr)):</strong> Rewards progress toward the goal, where d_prev and d_curr are previous and current distances to the goal.
                            </div>
                        </div>

                        <div class="reward-component">
                            <div class="symbol">r_goal</div>
                            <div class="description">
                                <strong>Goal Bonus (+500):</strong> Large positive reward upon reaching the goal within the specified threshold distance, incentivizing task completion.
                            </div>
                        </div>

                        <div class="reward-component">
                            <div class="symbol">r_coll</div>
                            <div class="description">
                                <strong>Collision Penalty (-100):</strong> Significant penalty for any collision with obstacles, teaching the agent to prioritize safety.
                            </div>
                        </div>

                        <div class="reward-component">
                            <div class="symbol">r_time</div>
                            <div class="description">
                                <strong>Time Penalty (-0.1):</strong> Small constant penalty at each timestep to encourage faster task completion and efficient trajectories.
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Neural Network Architecture -->
                <div class="project-section">
                    <h2><i data-lucide="network"></i> Neural Network Architecture</h2>
                    <p>
                        The policy is implemented using a hierarchical actor-critic architecture designed to support multiple temporally extended macro-actions (options). The network consists of shared layers for feature extraction, followed by modular heads.
                    </p>

                    <div class="feature-highlight">
                        <h3><i data-lucide="layers"></i> Network Structure</h3>
                        <ul style="color: var(--text-secondary); margin: 0; padding-left: 1.5rem; line-height: 2;">
                            <li><strong>Shared Backbone:</strong> Linear(state_dim, 128) → ReLU → Linear(128, 128) → ReLU</li>
                            <li><strong>Actor Heads (Per Option):</strong> Linear(128, action_dim) → Tanh (bounded actions [-1, 1])</li>
                            <li><strong>Critic Heads (Per Option):</strong> Linear(128, 1) for value estimation V(s)</li>
                            <li><strong>Option Selector:</strong> Linear(state_dim, 64) → ReLU → Linear(64, num_options)</li>
                        </ul>
                    </div>

                    <h3><i data-lucide="settings-2"></i> Training Configuration</h3>
                    <div class="methodology-grid" style="grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));">
                        <div class="methodology-card" style="padding: 1rem;">
                            <h4 style="font-size: 0.9rem; margin-bottom: 0.25rem;">Learning Rate</h4>
                            <p style="margin: 0; color: var(--accent-secondary); font-family: 'JetBrains Mono', monospace;">3×10⁻⁴</p>
                        </div>
                        <div class="methodology-card" style="padding: 1rem;">
                            <h4 style="font-size: 0.9rem; margin-bottom: 0.25rem;">Batch Size</h4>
                            <p style="margin: 0; color: var(--accent-secondary); font-family: 'JetBrains Mono', monospace;">64 / 128</p>
                        </div>
                        <div class="methodology-card" style="padding: 1rem;">
                            <h4 style="font-size: 0.9rem; margin-bottom: 0.25rem;">Discount (γ)</h4>
                            <p style="margin: 0; color: var(--accent-secondary); font-family: 'JetBrains Mono', monospace;">0.99</p>
                        </div>
                        <div class="methodology-card" style="padding: 1rem;">
                            <h4 style="font-size: 0.9rem; margin-bottom: 0.25rem;">GAE (λ)</h4>
                            <p style="margin: 0; color: var(--accent-secondary); font-family: 'JetBrains Mono', monospace;">0.95</p>
                        </div>
                        <div class="methodology-card" style="padding: 1rem;">
                            <h4 style="font-size: 0.9rem; margin-bottom: 0.25rem;">Clip (ε)</h4>
                            <p style="margin: 0; color: var(--accent-secondary); font-family: 'JetBrains Mono', monospace;">0.2</p>
                        </div>
                        <div class="methodology-card" style="padding: 1rem;">
                            <h4 style="font-size: 0.9rem; margin-bottom: 0.25rem;">Total Steps</h4>
                            <p style="margin: 0; color: var(--accent-secondary); font-family: 'JetBrains Mono', monospace;">10M</p>
                        </div>
                    </div>
                </div>

                <!-- Experimental Results -->
                <div class="project-section">
                    <h2><i data-lucide="bar-chart-2"></i> Experimental Results</h2>
                    <p>
                        We evaluated three approaches: PPO alone, PPO + SMDP, and the full hybrid PPO + SMDP + MPC. The results demonstrate that incorporating temporal abstraction and local trajectory refinement significantly improves task performance.
                    </p>

                    <div class="project-image-card" onclick="openImageModal('assets/rl-arm/report_img_7_1.png', 'Performance Comparison Across Strategies')" style="max-width: 800px; margin: 1.5rem auto;">
                        <img src="assets/rl-arm/report_img_7_1.png" alt="Performance comparison">
                        <div class="caption">Performance comparison across different control strategies - Hybrid approach achieves highest success rate and reward</div>
                    </div>

                    <h3><i data-lucide="table"></i> Performance Metrics</h3>
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Metric</th>
                                <th>PPO</th>
                                <th>PPO + SMDP</th>
                                <th>PPO + SMDP + MPC</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Success Rate (%)</strong></td>
                                <td>42</td>
                                <td>61</td>
                                <td class="highlight-row">83</td>
                            </tr>
                            <tr>
                                <td><strong>Average Reward</strong></td>
                                <td>67.2</td>
                                <td>98.7</td>
                                <td class="highlight-row">156.3</td>
                            </tr>
                            <tr>
                                <td><strong>Episode Length (steps)</strong></td>
                                <td>312</td>
                                <td>258</td>
                                <td class="highlight-row">213</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <!-- Learning Progress -->
                <div class="project-section">
                    <h2><i data-lucide="trending-up"></i> Learning Progress</h2>
                    <p>
                        The learning curves illustrate the training trajectory of each approach, showing how reward accumulates over timesteps. The hybrid approach demonstrates steady improvement and achieves the highest final performance.
                    </p>

                    <div class="project-image-card" onclick="openImageModal('assets/rl-arm/report_img_8_1.png', 'Hybrid Approach Learning Curve')" style="max-width: 800px; margin: 1.5rem auto;">
                        <img src="assets/rl-arm/report_img_8_1.png" alt="Learning progress">
                        <div class="caption">Learning progress of the PPO+SMDP+MPC approach showing steady improvement in reward</div>
                    </div>

                    <h3><i data-lucide="git-compare"></i> Ablation Studies</h3>
                    <p>
                        We conducted ablation studies to evaluate individual contributions of each component:
                    </p>

                    <div class="project-image-grid three-col">
                        <div class="project-image-card" onclick="openImageModal('assets/rl-arm/report_img_8_2.png', 'PPO Baseline Performance')">
                            <img src="assets/rl-arm/report_img_8_2.png" alt="PPO Performance">
                            <div class="caption">PPO Baseline - Gradual improvement from negative to positive rewards</div>
                        </div>
                        <div class="project-image-card" onclick="openImageModal('assets/rl-arm/report_img_9_1.png', 'PPO+SMDP Performance')">
                            <img src="assets/rl-arm/report_img_9_1.png" alt="PPO+SMDP Performance">
                            <div class="caption">PPO+SMDP - Higher rewards (~+20) with temporal abstraction</div>
                        </div>
                        <div class="project-image-card" onclick="openImageModal('assets/rl-arm/report_img_9_2.png', 'PPO+SMDP+MPC Performance')">
                            <img src="assets/rl-arm/report_img_9_2.png" alt="PPO+SMDP+MPC Performance">
                            <div class="caption">PPO+SMDP+MPC - Best performance (~+40) with full hybrid approach</div>
                        </div>
                    </div>

                    <div class="feature-highlight" style="margin-top: 1.5rem;">
                        <h3><i data-lucide="lightbulb"></i> Key Findings</h3>
                        <ul style="color: var(--text-secondary); margin: 0; padding-left: 1.5rem; line-height: 2;">
                            <li><strong>Temporal Abstraction (SMDP):</strong> Enables efficient exploration and smoother behavior through macro-actions</li>
                            <li><strong>Complementary Control (RL + MPC):</strong> RL excels at global planning while MPC ensures local precision</li>
                            <li><strong>Reward Design:</strong> Dense, well-structured rewards accelerate convergence and shape effective policies</li>
                            <li><strong>Curriculum Learning:</strong> Gradually increasing difficulty promotes stable learning and robust policies</li>
                        </ul>
                    </div>
                </div>

                <!-- Discussion -->
                <div class="project-section">
                    <h2><i data-lucide="message-circle"></i> Discussion & Future Work</h2>

                    <h3><i data-lucide="alert-triangle"></i> Limitations</h3>
                    <ul style="color: var(--text-secondary); margin: 0; padding-left: 1.5rem; line-height: 2;">
                        <li><strong>Simulation-to-Reality Gap:</strong> Transferring to real hardware requires addressing sensor noise, actuation delays, and environmental variability</li>
                        <li><strong>Computational Overhead:</strong> MPC module requires real-time trajectory optimization at each control step</li>
                        <li><strong>Task Specificity:</strong> Current implementation optimized for goal-reaching in static/semi-dynamic environments</li>
                    </ul>

                    <h3><i data-lucide="rocket"></i> Future Directions</h3>
                    <ul style="color: var(--text-secondary); margin: 0; padding-left: 1.5rem; line-height: 2;">
                        <li><strong>Visual State Representation:</strong> Incorporate RGB/depth images using CNNs for visually rich environments</li>
                        <li><strong>Multi-Task Learning:</strong> Extend to multiple manipulation tasks through shared representations</li>
                        <li><strong>Real-World Deployment:</strong> Address sim-to-real gap using domain randomization and safety constraints</li>
                    </ul>
                </div>

                <!-- Technologies -->
                <div class="project-section">
                    <h2><i data-lucide="wrench"></i> Technologies Used</h2>

                    <div class="tech-stack-grid">
                        <div class="tech-item">
                            <i data-lucide="code"></i>
                            <span>Python</span>
                        </div>
                        <div class="tech-item">
                            <i data-lucide="box"></i>
                            <span>PyBullet</span>
                        </div>
                        <div class="tech-item">
                            <i data-lucide="brain"></i>
                            <span>PyTorch</span>
                        </div>
                        <div class="tech-item">
                            <i data-lucide="bar-chart"></i>
                            <span>NumPy</span>
                        </div>
                        <div class="tech-item">
                            <i data-lucide="image"></i>
                            <span>Matplotlib</span>
                        </div>
                        <div class="tech-item">
                            <i data-lucide="activity"></i>
                            <span>OpenAI Gym</span>
                        </div>
                    </div>
                </div>

                <!-- Resources -->
                <div class="project-section">
                    <h2><i data-lucide="folder-open"></i> Project Resources</h2>
                    <p>Access the complete source code, documentation, and research report:</p>

                    <div class="cta-buttons" style="margin-top: 1rem;">
                        <a href="https://github.com/Adithya191101/RL_final_project" target="_blank" rel="noopener" class="btn-github">
                            <i data-lucide="github"></i>
                            GitHub Repository
                        </a>
                        <a href="assets/rl-arm/RL_Report.pdf" target="_blank" rel="noopener" class="btn-github">
                            <i data-lucide="file-text"></i>
                            Full Research Report
                        </a>
                        <a href="https://github.com/Barath-Balamurugan/ARL" target="_blank" rel="noopener" class="btn-github">
                            <i data-lucide="external-link"></i>
                            Main ARL Repository
                        </a>
                    </div>
                </div>

            </div>
        </section>
    </main>

    <!-- Image Modal -->
    <div class="certificate-modal" id="imageModal">
        <div class="modal-overlay" onclick="closeImageModal()"></div>
        <div class="modal-content image-modal-content">
            <button class="modal-close" onclick="closeImageModal()">
                <i data-lucide="x"></i>
            </button>
            <h3 class="modal-title" id="imageModalTitle">Image</h3>
            <img id="modalImage" src="" alt="Project Image">
        </div>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-links">
                    <a href="index.html#about">About</a>
                    <a href="projects.html">Projects</a>
                    <a href="contact.html">Contact</a>
                </div>

                <div class="footer-social">
                    <a href="https://linkedin.com/in/adithyarajendran" target="_blank" rel="noopener" aria-label="LinkedIn">
                        <i data-lucide="linkedin"></i>
                    </a>
                    <a href="https://github.com/Adithya191101" target="_blank" rel="noopener" aria-label="GitHub">
                        <i data-lucide="github"></i>
                    </a>
                    <a href="mailto:adithya191101@gmail.com" aria-label="Email">
                        <i data-lucide="mail"></i>
                    </a>
                </div>

                <p class="footer-copyright">
                    &copy; 2025 Adithya Rajendran. All rights reserved.
                </p>

                <p class="footer-tagline">
                    Built with passion for robotics
                </p>
            </div>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>
